---
title: "Aim One: Clustering"
author: "Shu Asai"
date: "11/09/2021"
output: pdf_document
indent: true
---

```{r, echo = TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
options(scipen = 999)
set.seed(1125)
```

```{r}
#read in training dataset
#train <- readRDS("../../../HRSdat_mice_rf01.rds")
fulltrain <- readRDS("../../../HRSdat_presplit_mice_rf01.rds")
#make copy of original data
tdf <- fulltrain
```

Since K is not clearly defined nor can it be abstracted confidently from the data, we implement hierarchical clustering.  

### Prepare data:  
```{r}
#http://www.econ.upf.edu/~michael/stanford/maeb7.pdf

#First: perform one-hot encoding on the factor variables
levels(fulltrain$retirement) <- c(1,0)
levels(fulltrain$sex) <- c(1,0)
levels(fulltrain$high_BP) <- c(1,0)
levels(fulltrain$diabetes) <- c(1,0)
levels(fulltrain$cancer) <- c(1,0)
levels(fulltrain$depression) <- c(1,0)
levels(fulltrain$lonely_pw) <- c(1,0)
levels(fulltrain$enjoy_life_pw) <- c(1,0)
levels(fulltrain$help_friends_py) <- c(1,0)
levels(fulltrain$have_friends) <- c(1,0)
levels(fulltrain$difficulty_managing_mny) <- c(1,0)
levels(fulltrain$social_security) <- c(1,0)
levels(fulltrain$medicare) <- c(1,0)
levels(fulltrain$life_insurance) <- c(1,0)

#take out the marital status and own_rent_home variables to create dummy variables
train_factor <- fulltrain %>% 
  select(marital_status, own_rent_home)
#create dummy variables
dumdat <- fastDummies::dummy_cols(train_factor) %>% 
  select(-marital_status, -own_rent_home)

#take the rest of the variables and encode into numbers since these have natural ordering
fulltrain <- fulltrain %>% 
  select(-marital_status, -own_rent_home) %>% 
  mutate(life_satisfaction = ifelse(life_satisfaction =='Not At All Satisfied', 0 ,
                                    ifelse(life_satisfaction =='Not Very Satisfied', 1, 
                                           ifelse(life_satisfaction =='Somewhat Satisfied', 2, 
                                                  ifelse(life_satisfaction == 'Very Satisfied', 3, 
                                                         ifelse(life_satisfaction =='Completely Satisfied', 4, NA))))), 
         health_rate = ifelse(health_rate =='Poor', 0 ,
                                    ifelse(health_rate =='Fair', 1, 
                                           ifelse(health_rate =='Good', 2, 
                                                  ifelse(health_rate == 'Very Good', 3, 
                                                         ifelse(health_rate =='Excellent', 4, NA))))), 
         memory = ifelse(memory =='Poor', 0 ,
                                    ifelse(memory =='Fair', 1, 
                                           ifelse(memory =='Good', 2, 
                                                  ifelse(memory == 'Very Good', 3, 
                                                         ifelse(memory =='Excellent', 4, NA))))), 
         follow_stockmarket= ifelse(follow_stockmarket =='Not At All', 0 ,
                                    ifelse(follow_stockmarket =='Somewhat Closely', 1, 
                                           ifelse(follow_stockmarket =='Very Closely', 2, NA)))) 

#make the dichotomous factor a numeric 
df_tr <- data.frame(lapply(fulltrain, function(x) as.numeric(as.character(x))))
#bind the two dataframes together
df_tr <- cbind(df_tr, dumdat)
#saveRDS(df_tr, file = "/home/guest/sem_3/707/health_retirement/model_matrix_training.RDS")
#then scale the data
df <- scale(df_tr)
#the data is now ready to be used for hierarchical clustering
```




### Hierarchical Clustering
  
```{r, fig.width=10, fig.height=8}
# Dissimilarity matrix
dismat <- dist(df, method = "euclidean")
#Hclust via Complete Linkage
hc_complete <- hclust(dismat, method = "complete" )
# Plot the obtained dendrogram
plot(hc_complete, cex = 0.6, hang = -1, main = "Cluster Dendrogram\nComplete Euclidean",
     xlab = "")
rect.hclust(hc_complete, k = 4)
```
  
  
```{r, fig.width=10, fig.height=8}
#Hclust via Average Linkage
hc_avg <- hclust(dismat, method = "average" )
# Plot the obtained dendrogram
plot(hc_avg, cex = 0.6, hang = -1, main = "Cluster Dendrogram\nAverage Euclidean",
     xlab = "")
rect.hclust(hc_avg, k = 4)
```

  
  
```{r, fig.width=10, fig.height=8}
#Hclust via Average Linkage
hc_single <- hclust(dismat, method = "single" )
# Plot the obtained dendrogram
plot(hc_single, cex = 0.6, hang = -1, main = "Cluster Dendrogram\nSingle Euclidean",
     xlab = "")
rect.hclust(hc_single, k = 4)
```
  
  
```{r, fig.width=10, fig.height=8}
ward_den <- hclust(dismat, method = "ward.D2")
#plot dendrogram 
plot(ward_den, cex = 0.6, hang = -1, main = "Cluster Dendrogram \nWard's k=4",
     xlab = "")
rect.hclust(ward_den, k = 4)
```



The clustering appears far more organized using the Ward's distance metric.  
We will use Ward's with four groupings.  

#### Groupings   
```{r}
# Cut tree into 4 groups
df_grp <- cutree(ward_den, k = 4)
#add these groupings to the data: 
tdf <- tdf %>% 
  mutate(cluster = df_grp)
```

#### Perform Analysis on the groupings:  
```{r}
tdf_analysis <- tdf %>% 
  group_by(cluster) %>%
  mutate(mean_age = mean(age), 
         mean_children = mean(children), 
         mean_netassets = mean(net_assets), 
         #rowcount = n(),
         prop_male = length(which(sex == "Male")) / n() , 
         prop_retired = length(which(retirement == "Retired")) / n(),
         prop_married = length(which(marital_status == "Married")) / n(),
         prop_satisfied = length(which(life_satisfaction %in% c("Very Satisfied", "Completely Satisfied"))) / n(),
         prop_health = length(which(health_rate %in% c("Good", "Very Good", "Excellent"))) / n()) %>%
  filter(row_number()==1) %>% 
  ungroup() %>% 
  select(cluster, mean_age, mean_children, mean_netassets, prop_male, prop_retired, 
         prop_married, prop_satisfied, prop_health) %>% 
  data.frame()

knitr::kable(
  tdf_analysis,
  col.names = c("Cluster",'Mean Age', 'Mean Children', 'Mean Net Assets', 'Prop Male',
                'Prop Retired', 'Prop Married','Prop Satisfied Life', 'Prop Good Health'),
  digits = 3
)
``` 

